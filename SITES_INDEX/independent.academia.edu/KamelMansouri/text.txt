Kamel Mansouri - Academia.edu
      Skip to main content
    
Academia.edu no longer supports Internet Explorer.To browse Academia.edu and the wider internet faster and more securely, please take a few seconds to upgrade your browser.
Log InSign UpLog InSign Upmore Job BoardAboutPressBlogPeoplePapersTermsPrivacyCopyright We're Hiring! Help Centerless 
Kamel MansouriChemistry+35 Followers3 Following2 Co-authorsTotal Views ;FollowFollowingPapersCATMoS: Collaborative Acute Toxicity Modeling SuiteEnvironmental Health PerspectivesSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
Application of new approach methodologies: ICE tools to support chemical evaluationsComputational ToxicologyAbstract New approach methodologies (NAMs) for toxicological applications such as in vitro assays... more Abstract New approach methodologies (NAMs) for toxicological applications such as in vitro assays and in silico models generate data that can be useful for assessing potential health impacts of chemicals. The National Toxicology Program’s (NTP’s) Integrated Chemical Environment (ICE; https://ice.ntp.niehs.nih.gov/ ) provides user-friendly access to NAM data and tools to explore and contextualize chemical bioactivity and molecular properties. ICE contains curated in vivo and in vitro toxicity testing data and experimental physicochemical property data gathered from different literature sources. ICE also contains computationally generated toxicity data and physicochemical parameter predictions. ICE provides interactive computational tools that characterize, analyze, and predict bioactivity for user-defined chemicals. ICE Search allows users to select and merge data sets for lists of chemicals and mixtures, yielding summary-level information, curated reference data, and bioactivity details mapped to mechanistic targets and modes of action. With the Curve Surfer tool, the user can explore concentration–response relationships of curated high-throughput screening assays. The Physiologically Based Pharmacokinetics (PBPK) tool predicts tissue-level concentrations resulting from in vivo doses, while the In Vitro–In Vivo Extrapolation (IVIVE) tool translates in vitro activity concentrations to equivalent in vivo dose estimates. The Chemical Characterization tool displays distributions of physicochemical properties, bioactivity- and structure-based projections, and consumer product use information. Chemical Quest, the newest ICE tool, allows users to search for structurally similar chemicals to a target chemical or substructure from within the extensive ICE database. Retrieved information on target chemicals and those with similar structures can then be used to query other ICE tools and datasets, greatly expanding data available to address the user’s question. ICE links to other NTP and U.S. Environmental Protection Agency data sources, expanding ICE’s capacity to examine chemicals based on physicochemical properties, bioactivity, and product use categories.Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
CoMPARA: Collaborative Modeling Project for Androgen Receptor ActivityEnvironmental Health PerspectivesSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
InterPred: a webtool to predict chemical autofluorescence and luminescence interferenceNucleic Acids ResearchHigh-throughput screening (HTS) research programs for drug development or chemical hazard assessm... more High-throughput screening (HTS) research programs for drug development or chemical hazard assessment are designed to screen thousands of molecules across hundreds of biological targets or pathways. Most HTS platforms use fluorescence and luminescence technologies, representing more than 70% of the assays in the US Tox21 research consortium. These technologies are subject to interferent signals largely explained by chemicals interacting with light spectrum. This phenomenon results in up to 5–10% of false positive results, depending on the chemical library used. Here, we present the InterPred webserver (version 1.0), a platform to predict such interference chemicals based on the first large-scale chemical screening effort to directly characterize chemical-assay interference, using assays in the Tox21 portfolio specifically designed to measure autofluorescence and luciferase inhibition. InterPred combines 17 quantitative structure activity relationship (QSAR) models built using optimiz...Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
An integrated chemical environment with tools for chemical safety testingToxicology in VitroSave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Selecting a minimal set of androgen receptor assays for screening chemicalsRegulatory Toxicology and PharmacologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
The TTC Data Mart: An interactive browser for threshold of toxicological concern calculationsComputational ToxicologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
High-Throughput Screening to Predict Chemical-Assay InterferenceScientific ReportsThe U.S. federal consortium on toxicology in the 21st century (Tox21) produces quantitative, high... more The U.S. federal consortium on toxicology in the 21st century (Tox21) produces quantitative, high-throughput screening (HTS) data on thousands of chemicals across a wide range of assays covering critical biological targets and cellular pathways. Many of these assays, and those used in other in vitro screening programs, rely on luciferase and fluorescence-based readouts that can be susceptible to signal interference by certain chemical structures resulting in false positive outcomes. Included in the Tox21 portfolio are assays specifically designed to measure interference in the form of luciferase inhibition and autofluorescence via multiple wavelengths (red, blue, and green) and under various conditions (cell-free and cell-based, two cell types). Out of 8,305 chemicals tested in the Tox21 interference assays, percent actives ranged from 0.5% (red autofluorescence) to 9.9% (luciferase inhibition). Self-organizing maps and hierarchical clustering were used to relate chemical structural...Save to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
The role of fit-for-purpose assays within tiered testing approaches: A case study evaluating prioritized estrogen-active compounds in an in vitro human uterotrophic assayToxicology and Applied PharmacologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Open-source QSAR models for pKa prediction using multiple machine learning approachesJournal of CheminformaticsBackground The logarithmic acid dissociation constant pKa reflects the ionization of a chemical, ... more Background The logarithmic acid dissociation constant pKa reflects the ionization of a chemical, which affects lipophilicity, solubility, protein binding, and ability to pass through the plasma membrane. Thus, pKa affects chemical absorption, distribution, metabolism, excretion, and toxicity properties. Multiple proprietary software packages exist for the prediction of pKa, but to the best of our knowledge no free and open-source programs exist for this purpose. Using a freely available data set and three machine learning approaches, we developed open-source models for pKa prediction. Methods The experimental strongest acidic and strongest basic pKa values in water for 7912 chemicals were obtained from DataWarrior, a freely available software package. Chemical structures were curated and standardized for quantitative structure–activity relationship (QSAR) modeling using KNIME, and a subset comprising 79% of the initial set was used for modeling. To evaluate different approaches to m...Save to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
SAR and QSAR modeling of a large collection of LD50 rat acute oral toxicity dataJournal of CheminformaticsSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
Addressing systematic inconsistencies between in vitro and in vivo transcriptomic mode of action signaturesToxicology in VitroSave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Development, validation and integration of in silico models to identify androgen active chemicalsChemosphereSave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
{"__content__"=>"A Workflow for Identifying Metabolically Active Chemicals to Complement Toxicity Screening.", "i"=>{"__content__"=>"in vitro"}}Computational toxicology (Amsterdam, Netherlands),  2018The new paradigm of toxicity testing approaches involves rapid screening of thousands of chemical... more The new paradigm of toxicity testing approaches involves rapid screening of thousands of chemicals across hundreds of biological targets through use of assays. Such assays may lead to false negatives when the complex metabolic processes that render a chemical bioactive in a living system are unable to be replicated in an environment. In the current study, a workflow is presented for complementing testing results with and techniques to identify inactive parents that may produce active metabolites. A case study applying this workflow involved investigating the influence of metabolism for over 1,400 chemicals considered inactive across18 assays related to the estrogen receptor (ER) pathway. Over 7,500 first-generation and second-generation metabolites were generated for these inactive chemicals using an software program. Next, a consensus model comprised of four individual quantitative structure activity relationship (QSAR) models was used to predict ER-binding activity for each of the...Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Prediction of acute oral systemic toxicity using a multi-fingerprint similarity approachToxicological sciences : an official journal of the Society of Toxicology,  Jan 29, 2018The implementation of non-animal approaches is of particular importance to regulatory agencies fo... more The implementation of non-animal approaches is of particular importance to regulatory agencies for the prediction of potential hazards associated with acute exposures to chemicals. This work was carried in the framework of an international modeling initiative organized by the Acute Toxicity Workgroup (ATWG) of the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM) with the participation of thirty-two international groups across government, industry and academia. Our contribution was to develop a multi-fingerprints similarity approach for predicting five relevant toxicology endpoints related to the acute oral systemic toxicity that are: the median lethal dose (LD50) point prediction, the &quot;nontoxic&quot; (LD50&gt;2000 mg/kg) and &quot;very toxic&quot; (LD50&lt;50 mg/kg) binary classification and the multi-class categorization of chemicals based on the United States Environmental Protection Agency and Globally Harmonized System of Classification a...Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
EPA’s non-targeted analysis collaborative trial (ENTACT): genesis, design, and initial findingsAnalytical and Bioanalytical ChemistryIn August 2015, the US Environmental Protection Agency (EPA) convened a workshop entitled &amp;qu... more In August 2015, the US Environmental Protection Agency (EPA) convened a workshop entitled &amp;quot;Advancing non-targeted analyses of xenobiotic chemicals in environmental and biological media.&amp;quot; The purpose of the workshop was to bring together the foremost experts in non-targeted analysis (NTA) to discuss the state-of-the-science for generating, interpreting, and exchanging NTA measurement data. During the workshop, participants discussed potential designs for a collaborative project that would use EPA resources, including the ToxCast library of chemical substances, the DSSTox database, and the CompTox Chemicals Dashboard, to evaluate cutting-edge NTA methods. That discussion was the genesis of EPA&amp;#39;s Non-Targeted Analysis Collaborative Trial (ENTACT). Nearly 30 laboratories have enrolled in ENTACT and used a variety of chromatography, mass spectrometry, and data processing approaches to characterize ten synthetic chemical mixtures, three standardized media (human serum, house dust, and silicone band) extracts, and thousands of individual substances. Initial results show that nearly all participants have detected and reported more compounds in the mixtures than were intentionally added, with large inter-lab variability in the number of reported compounds. A comparison of gas and liquid chromatography results shows that the majority (45.3%) of correctly identified compounds were detected by only one method and 15.4% of compounds were not identified. Finally, a limited set of true positive identifications indicates substantial differences in observable chemical space when employing disparate separation and ionization techniques as part of NTA workflows. This article describes the genesis of ENTACT, all study methods and materials, and an analysis of results submitted to date. Graphical abstract ᅟ.Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
“MS-Ready” structures for non-targeted high-resolution mass spectrometry screening studiesJournal of CheminformaticsSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
A Qualitative Modeling Approach for Whole Genome Prediction Using High-Throughput Toxicogenomics Data and Pathway-Based ValidationFrontiers in pharmacology,  2018Efficient high-throughput transcriptomics (HTT) tools promise inexpensive, rapid assessment of po... more Efficient high-throughput transcriptomics (HTT) tools promise inexpensive, rapid assessment of possible biological consequences of human and environmental exposures to tens of thousands of chemicals in commerce. HTT systems have used relatively small sets of gene expression measurements coupled with mathematical prediction methods to estimate genome-wide gene expression and are often trained and validated using pharmaceutical compounds. It is unclear whether these training sets are suitable for general toxicity testing applications and the more diverse chemical space represented by commercial chemicals and environmental contaminants. In this work, we built predictive computational models that inferred whole genome transcriptional profiles from a smaller sample of surrogate genes. The model was trained and validated using a large scale toxicogenomics database with gene expression data from exposure to heterogeneous chemicals from a wide range of classes (the Open TG-GATEs data base)....Save to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
New approach methods for testing chemicals for endocrine disruption potentialCurrent Opinion in ToxicologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Predictive models for acute oral systemic toxicity: A workshop to bridge the gap from research to regulationComputational ToxicologyIn early 2018, the Interagency Coordinating Committee for the Validation of Alternative Methods (... more In early 2018, the Interagency Coordinating Committee for the Validation of Alternative Methods (ICCVAM) published the &amp;quot;Strategic Roadmap for Establishing New Approaches to Evaluate the Safety of Chemicals and Medical Products in the United States&amp;quot; (ICCVAM 2018). Cross-agency federal workgroups have been established to implement this roadmap for various toxicological testing endpoints, with an initial focus on acute toxicity testing. The ICCVAM acute toxicity workgroup (ATWG) helped organize a global collaboration to build predictive in silico models for acute oral systemic toxicity, based on a large dataset of rodent studies and targeted towards regulatory needs identified across federal agencies. Thirty-two international groups across government, industry, and academia participated in the project, culminating in a workshop in April 2018 held at the National Institutes of Health (NIH). At the workshop, computational modelers and regulatory decision makers met to discuss the feasibility of using predictive model outputs for regulatory use in lieu of acute oral systemic toxicity testing. The models were combined to yield consensus predictions which demonstrated excellent performance when compared to the animal data, and workshop outcomes and follow-up activities to make these tools available and put them into practice are discussed here.Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
CATMoS: Collaborative Acute Toxicity Modeling SuiteEnvironmental Health PerspectivesSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
Application of new approach methodologies: ICE tools to support chemical evaluationsComputational ToxicologyAbstract New approach methodologies (NAMs) for toxicological applications such as in vitro assays... more Abstract New approach methodologies (NAMs) for toxicological applications such as in vitro assays and in silico models generate data that can be useful for assessing potential health impacts of chemicals. The National Toxicology Program’s (NTP’s) Integrated Chemical Environment (ICE; https://ice.ntp.niehs.nih.gov/ ) provides user-friendly access to NAM data and tools to explore and contextualize chemical bioactivity and molecular properties. ICE contains curated in vivo and in vitro toxicity testing data and experimental physicochemical property data gathered from different literature sources. ICE also contains computationally generated toxicity data and physicochemical parameter predictions. ICE provides interactive computational tools that characterize, analyze, and predict bioactivity for user-defined chemicals. ICE Search allows users to select and merge data sets for lists of chemicals and mixtures, yielding summary-level information, curated reference data, and bioactivity details mapped to mechanistic targets and modes of action. With the Curve Surfer tool, the user can explore concentration–response relationships of curated high-throughput screening assays. The Physiologically Based Pharmacokinetics (PBPK) tool predicts tissue-level concentrations resulting from in vivo doses, while the In Vitro–In Vivo Extrapolation (IVIVE) tool translates in vitro activity concentrations to equivalent in vivo dose estimates. The Chemical Characterization tool displays distributions of physicochemical properties, bioactivity- and structure-based projections, and consumer product use information. Chemical Quest, the newest ICE tool, allows users to search for structurally similar chemicals to a target chemical or substructure from within the extensive ICE database. Retrieved information on target chemicals and those with similar structures can then be used to query other ICE tools and datasets, greatly expanding data available to address the user’s question. ICE links to other NTP and U.S. Environmental Protection Agency data sources, expanding ICE’s capacity to examine chemicals based on physicochemical properties, bioactivity, and product use categories.Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
CoMPARA: Collaborative Modeling Project for Androgen Receptor ActivityEnvironmental Health PerspectivesSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
InterPred: a webtool to predict chemical autofluorescence and luminescence interferenceNucleic Acids ResearchHigh-throughput screening (HTS) research programs for drug development or chemical hazard assessm... more High-throughput screening (HTS) research programs for drug development or chemical hazard assessment are designed to screen thousands of molecules across hundreds of biological targets or pathways. Most HTS platforms use fluorescence and luminescence technologies, representing more than 70% of the assays in the US Tox21 research consortium. These technologies are subject to interferent signals largely explained by chemicals interacting with light spectrum. This phenomenon results in up to 5–10% of false positive results, depending on the chemical library used. Here, we present the InterPred webserver (version 1.0), a platform to predict such interference chemicals based on the first large-scale chemical screening effort to directly characterize chemical-assay interference, using assays in the Tox21 portfolio specifically designed to measure autofluorescence and luciferase inhibition. InterPred combines 17 quantitative structure activity relationship (QSAR) models built using optimiz...Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
An integrated chemical environment with tools for chemical safety testingToxicology in VitroSave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Selecting a minimal set of androgen receptor assays for screening chemicalsRegulatory Toxicology and PharmacologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
The TTC Data Mart: An interactive browser for threshold of toxicological concern calculationsComputational ToxicologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
High-Throughput Screening to Predict Chemical-Assay InterferenceScientific ReportsThe U.S. federal consortium on toxicology in the 21st century (Tox21) produces quantitative, high... more The U.S. federal consortium on toxicology in the 21st century (Tox21) produces quantitative, high-throughput screening (HTS) data on thousands of chemicals across a wide range of assays covering critical biological targets and cellular pathways. Many of these assays, and those used in other in vitro screening programs, rely on luciferase and fluorescence-based readouts that can be susceptible to signal interference by certain chemical structures resulting in false positive outcomes. Included in the Tox21 portfolio are assays specifically designed to measure interference in the form of luciferase inhibition and autofluorescence via multiple wavelengths (red, blue, and green) and under various conditions (cell-free and cell-based, two cell types). Out of 8,305 chemicals tested in the Tox21 interference assays, percent actives ranged from 0.5% (red autofluorescence) to 9.9% (luciferase inhibition). Self-organizing maps and hierarchical clustering were used to relate chemical structural...Save to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
The role of fit-for-purpose assays within tiered testing approaches: A case study evaluating prioritized estrogen-active compounds in an in vitro human uterotrophic assayToxicology and Applied PharmacologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Open-source QSAR models for pKa prediction using multiple machine learning approachesJournal of CheminformaticsBackground The logarithmic acid dissociation constant pKa reflects the ionization of a chemical, ... more Background The logarithmic acid dissociation constant pKa reflects the ionization of a chemical, which affects lipophilicity, solubility, protein binding, and ability to pass through the plasma membrane. Thus, pKa affects chemical absorption, distribution, metabolism, excretion, and toxicity properties. Multiple proprietary software packages exist for the prediction of pKa, but to the best of our knowledge no free and open-source programs exist for this purpose. Using a freely available data set and three machine learning approaches, we developed open-source models for pKa prediction. Methods The experimental strongest acidic and strongest basic pKa values in water for 7912 chemicals were obtained from DataWarrior, a freely available software package. Chemical structures were curated and standardized for quantitative structure–activity relationship (QSAR) modeling using KNIME, and a subset comprising 79% of the initial set was used for modeling. To evaluate different approaches to m...Save to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
SAR and QSAR modeling of a large collection of LD50 rat acute oral toxicity dataJournal of CheminformaticsSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
Addressing systematic inconsistencies between in vitro and in vivo transcriptomic mode of action signaturesToxicology in VitroSave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Development, validation and integration of in silico models to identify androgen active chemicalsChemosphereSave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
{"__content__"=>"A Workflow for Identifying Metabolically Active Chemicals to Complement Toxicity Screening.", "i"=>{"__content__"=>"in vitro"}}Computational toxicology (Amsterdam, Netherlands),  2018The new paradigm of toxicity testing approaches involves rapid screening of thousands of chemical... more The new paradigm of toxicity testing approaches involves rapid screening of thousands of chemicals across hundreds of biological targets through use of assays. Such assays may lead to false negatives when the complex metabolic processes that render a chemical bioactive in a living system are unable to be replicated in an environment. In the current study, a workflow is presented for complementing testing results with and techniques to identify inactive parents that may produce active metabolites. A case study applying this workflow involved investigating the influence of metabolism for over 1,400 chemicals considered inactive across18 assays related to the estrogen receptor (ER) pathway. Over 7,500 first-generation and second-generation metabolites were generated for these inactive chemicals using an software program. Next, a consensus model comprised of four individual quantitative structure activity relationship (QSAR) models was used to predict ER-binding activity for each of the...Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Prediction of acute oral systemic toxicity using a multi-fingerprint similarity approachToxicological sciences : an official journal of the Society of Toxicology,  Jan 29, 2018The implementation of non-animal approaches is of particular importance to regulatory agencies fo... more The implementation of non-animal approaches is of particular importance to regulatory agencies for the prediction of potential hazards associated with acute exposures to chemicals. This work was carried in the framework of an international modeling initiative organized by the Acute Toxicity Workgroup (ATWG) of the Interagency Coordinating Committee on the Validation of Alternative Methods (ICCVAM) with the participation of thirty-two international groups across government, industry and academia. Our contribution was to develop a multi-fingerprints similarity approach for predicting five relevant toxicology endpoints related to the acute oral systemic toxicity that are: the median lethal dose (LD50) point prediction, the &quot;nontoxic&quot; (LD50&gt;2000 mg/kg) and &quot;very toxic&quot; (LD50&lt;50 mg/kg) binary classification and the multi-class categorization of chemicals based on the United States Environmental Protection Agency and Globally Harmonized System of Classification a...Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
EPA’s non-targeted analysis collaborative trial (ENTACT): genesis, design, and initial findingsAnalytical and Bioanalytical ChemistryIn August 2015, the US Environmental Protection Agency (EPA) convened a workshop entitled &amp;qu... more In August 2015, the US Environmental Protection Agency (EPA) convened a workshop entitled &amp;quot;Advancing non-targeted analyses of xenobiotic chemicals in environmental and biological media.&amp;quot; The purpose of the workshop was to bring together the foremost experts in non-targeted analysis (NTA) to discuss the state-of-the-science for generating, interpreting, and exchanging NTA measurement data. During the workshop, participants discussed potential designs for a collaborative project that would use EPA resources, including the ToxCast library of chemical substances, the DSSTox database, and the CompTox Chemicals Dashboard, to evaluate cutting-edge NTA methods. That discussion was the genesis of EPA&amp;#39;s Non-Targeted Analysis Collaborative Trial (ENTACT). Nearly 30 laboratories have enrolled in ENTACT and used a variety of chromatography, mass spectrometry, and data processing approaches to characterize ten synthetic chemical mixtures, three standardized media (human serum, house dust, and silicone band) extracts, and thousands of individual substances. Initial results show that nearly all participants have detected and reported more compounds in the mixtures than were intentionally added, with large inter-lab variability in the number of reported compounds. A comparison of gas and liquid chromatography results shows that the majority (45.3%) of correctly identified compounds were detected by only one method and 15.4% of compounds were not identified. Finally, a limited set of true positive identifications indicates substantial differences in observable chemical space when employing disparate separation and ionization techniques as part of NTA workflows. This article describes the genesis of ENTACT, all study methods and materials, and an analysis of results submitted to date. Graphical abstract ᅟ.Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
“MS-Ready” structures for non-targeted high-resolution mass spectrometry screening studiesJournal of CheminformaticsSave to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
A Qualitative Modeling Approach for Whole Genome Prediction Using High-Throughput Toxicogenomics Data and Pathway-Based ValidationFrontiers in pharmacology,  2018Efficient high-throughput transcriptomics (HTT) tools promise inexpensive, rapid assessment of po... more Efficient high-throughput transcriptomics (HTT) tools promise inexpensive, rapid assessment of possible biological consequences of human and environmental exposures to tens of thousands of chemicals in commerce. HTT systems have used relatively small sets of gene expression measurements coupled with mathematical prediction methods to estimate genome-wide gene expression and are often trained and validated using pharmaceutical compounds. It is unclear whether these training sets are suitable for general toxicity testing applications and the more diverse chemical space represented by commercial chemicals and environmental contaminants. In this work, we built predictive computational models that inferred whole genome transcriptional profiles from a smaller sample of surrogate genes. The model was trained and validated using a large scale toxicogenomics database with gene expression data from exposure to heterogeneous chemicals from a wide range of classes (the Open TG-GATEs data base)....Save to LibraryDownloadEditCompare Citation Rank Readers Related Papers MentionsView Impact
New approach methods for testing chemicals for endocrine disruption potentialCurrent Opinion in ToxicologySave to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
Predictive models for acute oral systemic toxicity: A workshop to bridge the gap from research to regulationComputational ToxicologyIn early 2018, the Interagency Coordinating Committee for the Validation of Alternative Methods (... more In early 2018, the Interagency Coordinating Committee for the Validation of Alternative Methods (ICCVAM) published the &amp;quot;Strategic Roadmap for Establishing New Approaches to Evaluate the Safety of Chemicals and Medical Products in the United States&amp;quot; (ICCVAM 2018). Cross-agency federal workgroups have been established to implement this roadmap for various toxicological testing endpoints, with an initial focus on acute toxicity testing. The ICCVAM acute toxicity workgroup (ATWG) helped organize a global collaboration to build predictive in silico models for acute oral systemic toxicity, based on a large dataset of rodent studies and targeted towards regulatory needs identified across federal agencies. Thirty-two international groups across government, industry, and academia participated in the project, culminating in a workshop in April 2018 held at the National Institutes of Health (NIH). At the workshop, computational modelers and regulatory decision makers met to discuss the feasibility of using predictive model outputs for regulatory use in lieu of acute oral systemic toxicity testing. The models were combined to yield consensus predictions which demonstrated excellent performance when compared to the animal data, and workshop outcomes and follow-up activities to make these tools available and put them into practice are discussed here.Save to LibraryEditCompare Citation Rank Readers Related Papers MentionsView Impact
×CloseLog InLog in with FacebookLog in with GoogleorEmailPasswordRemember me on this computeror reset passwordEnter the email address you signed up with and we'll email you a reset link.
Need an account? Click here to sign up
AboutPressBlogPeoplePapersTopicsAcademia BiologyAcademia EngineeringAcademia MedicineJob Board We're Hiring! Help CenterFind new research papers in:PhysicsChemistryBiologyHealth SciencesEcologyEarth SciencesCognitive ScienceMathematicsComputer ScienceTermsPrivacyCopyrightAcademia ©2023