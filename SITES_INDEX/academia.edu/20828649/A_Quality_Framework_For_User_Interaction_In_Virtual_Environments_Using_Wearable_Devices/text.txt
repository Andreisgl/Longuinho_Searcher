(PDF) A Quality Framework For User Interaction In Virtual Environments Using Wearable Devices | Mahdi Babaei - Academia.edu
Academia.edu no longer supports Internet Explorer.To browse Academia.edu and the wider internet faster and more securely, please take a few seconds to upgrade your browser.
×CloseLog InLog in with FacebookLog in with GoogleorEmailPasswordRemember me on this computeror reset passwordEnter the email address you signed up with and we'll email you a reset link.
Need an account? Click here to sign up
Log InSign UpLog InSign UpmoreJob BoardAboutPressBlogPeoplePapersTermsPrivacyCopyrightWe're Hiring!Help Centerless
Download Free PDFDownload Free PDFA Quality Framework For User Interaction In Virtual Environments Using Wearable DevicesA Quality Framework For User Interaction In Virtual Environments Using Wearable DevicesA Quality Framework For User Interaction In Virtual Environments Using Wearable DevicesA Quality Framework For User Interaction In Virtual Environments Using Wearable DevicesA Quality Framework For User Interaction In Virtual Environments Using Wearable DevicesMahdi Babaei— Gesture recognition has been considered as one of the most effective input methods to interact with virtual environments (VEs). The skeleton tracking techniques which have been widely used for gesture recognition purposes showed common accuracy issues with Micro-gestures that can affect user's enjoyment. In this paper, we propose a multimodal interaction technique and test it using a designed wearable head-mounted tracker as a measurement instrument. We also designed a theoretical framework to resolve the weaknesses of Micro-Gestures recognition in CAVE (Cave Automatic Virtual Environment). The effectiveness of proposed method and its impact of user's joyfulness has been tested by a 3D gesture-based interface. The results showed improvement in user's enjoyment using the designed measuring and input method for navigation within a 3D CAVE by improving system's accuracy.See Full PDFDownload PDFSee Full PDFDownload PDFRelated Papers3DTouch: Towards a Wearable 3D Input Device for 3D ApplicationsAnh NguyenThree-dimensional (3D) applications have come to every corner of life. We present 3DTouch, a novel 3D wearable input device worn on the fingertip for interacting with 3D applications. 3DTouch is self-contained, and designed to universally work on various 3D platforms. The device employs touch input for the benefits of passive haptic feedback, and movement stability. Moreover, with touch interaction, 3DTouch is conceptually less fatiguing to use over many hours than 3D spatial input devices such as Kinect. Our approach relies on relative positioning technique using an optical laser sensor and a 9-DOF inertial measurement unit. We implemented a set of 3D interaction techniques including selection, translation, and rotation using 3DTouch. An evaluation also demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for subtle touch interaction in 3D space. With 3DTouch project, we would like to provide an input device that reduces the gap between 3D applications and users.Download Free PDFView PDFA Quality Framework for Multimodal Interaction in Educational EnvironmentsHossein Reza Babaei, Mahdi Babaei— Designing gesture-based collaborative and interactive systems using new technologies has shown noticeable potential for educational purposes and could enhance the memorability of digital contents. In this paper, we present a quality framework and a new combination of modalities to recognise different types of human gestures. The outcome can be used in future classroom designs by replacing expensive smart-boards which are physically limited to a fixed place. We also propose a framework based on Norman's theory of action for interactive collaborative systems. This framework has been tested by a combination of Microsoft Kinect and a smartphone's built-in gyroscope together with two types of interfaces in an educational environment. The results proved the improvement of participant's interactivity and participation by changes in five pedagogical factors in different groups of the study and specifically in higher ages.Download Free PDFView PDF3DTouch: A wearable 3D input device for 3D applicationsAnh Nguyen3D applications appear in every corner of life in the current technology era. There is a need for an ubiquitous 3D input device that works with many different platforms, from head-mounted displays (HMDs) to mobile touch devices, 3DTVs, and even the Cave Automatic Virtual Environments. We present 3DTouch, a novel wear-able 3D input device worn on the fingertip for 3D manipulation tasks. 3DTouch is designed to fill the missing gap of a 3D input device that is self-contained, mobile, and universally works across various 3D platforms. This paper presents a low-cost solution to designing and implementing such a device. Our approach relies on a relative positioning technique using an optical laser sensor and a 9-DOF inertial measurement unit. The device employs touch input for the benefits of passive haptic feedback, and movement stability. On the other hand, with touch interaction, 3DTouch is conceptually less fatiguing to use over many hours than 3D spatial input devices. We propose a set of 3D interaction techniques including selection, translation, and rotation using 3DTouch. An evaluation also demonstrates the device's tracking accuracy of 1.10 mm and 2.33 degrees for subtle touch interaction in 3D space. We envision that modular solutions like 3DTouch opens up a whole new design space for interaction techniques to further develop on. With 3DTouch, we attempt to bring 3D applications a step closer to users.Download Free PDFView PDFProceedings of the 2012 ACM annual conference extended abstracts on Human Factors in Computing Systems Extended Abstracts - CHI EA '12The 3rd dimension of CHI (3DCHI)2012 • Hrvoje BenkoDownload Free PDFView PDFPublished_JVR_Martin_Jude.pdfSagayam MartinDownload Free PDFView PDFShamanic Interface for computers and gaming platforms  by Filipe Miguel Alves Bandeira Pinto de CarvalhoDevrim CKDownload Free PDFView PDFInternational Journal of Virtual World and Human Computer InteractionA Prototyping Method to Simulate Wearable Augmented Reality Interaction in a Virtual Environment - A Pilot Study2015 • Klas  Hermodsson, Günter Alce, Tarik HadzovicDownload Free PDFView PDFMaster thesis: Usability Evaluation of the Kinect in Aiding Surgeon Computer Interaction2013 • Sebastiaan M StuijGesture-based interaction in the operating room can provide surgeons with a way to interact with medical images of the patient in a direct and sterile way as opposed to instructing an assistant. The purpose of this study was to determine whether a modern gesture-based interface using the Kinect is considered desirable and feasible during surgical procedures. To this end, we conducted a usability evaluation on a gesture-controlled medical image viewer with surgeons in a controlled operating room environment in the UMCG Skills Lab. Participants indicated that they would like to incorporate the system in the OR, even though performance measures indicated that the tested system is less accurate and slower when compared to asking an assistant. In a second study, we evaluated two popular gesture-based selection techniques (‘Dwell’ and ‘Push’) because of the importance of selection time and accuracy in surgical tasks. Results from this experiment indicated that the tested techniques were moderately less accurate while selection time was higher for gesture-based selection techniques when compared to the mouse condition. Overall, our studies show that GBI in the operating room is promising but possible refinements include additional functionality, recognition accuracy, more precise cursor control and more ‘natural’ gestures for certain functionality.Download Free PDFView PDFFraming Embodiment in General-Purpose Computing: a study identifying key components in a multimodal general-purpose computational environmentElisabeth NesheimThe last thirty years have presented us with technology that has had a profound impact on how we produce, socialize with others, and consume culture. Today most of these actions are linked to a computational setup which involves a screen representing our options in two dimensions and a hand-operated controller for manipulating the screen environment, a hardware setup that has not changed considerably the last 50 years. The dominant interface for personal computers—the graphical user interface—is highly ocularcentric, where only parts of the body apparatus (eyes and hands) are addressed in the interface directly. As an increasing amount of information, life experience and human contact is channeled through it, the desktop computer system, becomes increasingly inadequate to fully represent these actions. Any prosthesis added to or used in conjunction with the body and any part of the sensory apparatus neglected will define our interaction with information. Information gathered by the somesthetic—the touch and proprioceptic senses—constitute a significant component in the way we form hypotheses about what an object is, and how it can be manipulated. By addressing the somesthetic senses in computer interfaces, we can achieve richer and more intuitive interactive experiences. This paper aims to identify the key components of a general purpose computational environment that foreground multimodal interaction by 1) investigating the significant qualities of the somesthetic senses from a phenomenological and neurophysiological point of view, 2) pointing to successful principles of human computer interaction (coupling), and tools for designing embodied interactions (physical metaphors, interface agents, affordances, and visual and haptic feedback), 3) evaluating the components of current mobile phone technology, surface computing, responsive environments, and wearable computing. Strategies and plans of dominant technology companies strongly influence what interfaces and devices are available via the commercial market, turning many of us into passive users accepting the default setup made available to us. But if we can move beyond current ideas of what a computer is, re-invent and retell the stories of what we want living with a computer to be like, users are in a unique position to initiate and engage in discussions that influence artists, programmers, developers and engineers into trying something new.Download Free PDFView PDFOn the utility of 3D hand cursors to explore medical volume datasets with a touchless interfaceDaniel Simões  Lopes, Soraia Paulo, Paulo Amaral Rego, Manuel Cassiano  NevesAnalyzing medical volume datasets requires interactive visualization so that users can extract anatomo-physiological information in real-time. Conventional volume rendering systems rely on 2D input devices, such as mice and keyboards, which are known to hamper 3D analysis as users often struggle to obtain the desired orientation that is only achieved after several attempts. In this paper, we address which 3D analysis tools are better performed with 3D hand cursors operating on a touchless interface comparatively to a 2D input devices running on a conventional WIMP interface. The main goals of this paper are to explore the capabilities of (simple) hand gestures to facilitate sterile manipulation of 3D medical data on a touchless interface, without resorting on wearables, and to evaluate the surgical feasibility of the proposed interface next to senior surgeons (N = 5) and interns (N = 2). To this end, we developed a touchless interface controlled via hand gestures and body postures to rapidly rotate and position medical volume images in three-dimensions, where each hand acts as an interactive 3D cursor. User studies were conducted with laypeople, while informal evaluation sessions were carried with senior surgeons, radiologists and professional biomedical engineers. Results demonstrate its usability as the proposed touchless interface improves spatial awareness and a more fluent interaction with the 3D volume than with traditional 2D input devices, as it requires lesser number of attempts to achieve the desired orientation by avoiding the composition of several cumulative rotations, which is typically necessary in WIMP interfaces. However, tasks requiring precision such as clipping plane visualization and tagging are best performed with mouse-based systems due to noise, incorrect gestures detection and problems in skeleton tracking that need to be addressed before tests in real medical environments might be performed.Download Free PDFView PDFSee Full PDFDownload PDFLoading PreviewSorry, preview is currently unavailable. You can download the paper by clicking the button above.RELATED PAPERSLightweight palm and finger tracking for real-time 3D gesture control2011 • Rod McCallDownload Free PDFView PDFFreeze view touch and finger gesture based interaction methods for handheld augmented reality interfaces2012 • Gun  LeeDownload Free PDFView PDFVirtual Architecture in a Real-time, Interactive, Augmented Reality Environment Project Anywhere and the Potential of Architecture in the Age of the VirtualConstantinos MiltiadisDownload Free PDFView PDFECMS 2013 Proceedings edited by: Webjorn Rekdalsbakken, Robin T. Bye, Houxiang ZhangKinect-Based Systems For Maritime Operation Simulators?2013 • Arne G StyveDownload Free PDFView PDFNovel uses of Pinch Gloves™ for virtual environment interaction techniques2002 • Doug BowmanDownload Free PDFView PDF10th IEEE International Conference on Advanced Video and Signal-Based SurveillanceImprovement of the real-time gesture analysis by a new mother wavelet and the application for the navigation inside a scale-one 3D system 2013 • Jean-Rémy ChardonnetDownload Free PDFView PDFfreehand 3D interactions anywhere using a wrist-worn gloveless sensor2012 • Otmar HilligesDownload Free PDFView PDFFirst Person Movement Control with Palm Normal and Hand Gesture Interaction in Virtual RealityChaowanan KhundamDownload Free PDFView PDFGMDForschungszentrum Informationstechnik …Interaction for Virtual Environments based on Facial Expression Recognition2001 • Homero Rios-figueroaDownload Free PDFView PDFLecture Notes in Computer ScienceKinect vs. Low-cost Inertial Sensing for Gesture Recognition2014 • Francois DestelleDownload Free PDFView PDFGesture Based Interaction: Leap Motion Gesture Based Interaction: Leap MotionManny  ForsonDownload Free PDFView PDFA WEARABLE HAPTIC GAME CONTROLLERInternational Journal of Game Theory and Technology  ( IJGTT), Stefan MarksDownload Free PDFView PDFTiles: Toward Generic and Consistent MR Interfaces2001 • Mark BillinghurstDownload Free PDFView PDFA review of state of the art and emerging interaction technologies and their impact on design and designing in the future2012 • Philip CashDownload Free PDFView PDFGMD–Forschungszentrum Informationstechnik GmbHImmersive Multimodal 3D Interfaces for Music, Stories and Games2001 • Stephen BarrassDownload Free PDFView PDFCreepy Tracker Toolkit for Context-aware InterfacesRafael K Dos Anjos, Daniel MendesDownload Free PDFView PDFUnconventional 3D User Interfaces for Virtual EnvironmentsErnst KruijffDownload Free PDFView PDF2013 23rd International Conference on Artificial Reality and Telexistence (ICAT)Ultrasonic glove input device for distance-based interactions2013 • Ross SmithDownload Free PDFView PDFInternational Symposium on Safety, Security and Rescue Robotics (SSRR)New Interaction Metaphors to Control a Hydraulic Working Machine’s Arm2016 • Matteo RagagliaDownload Free PDFView PDFInternational Journal of Networked Computing and Advanced Information Management (IJNCM)Stepping into Augmented Reality2011 • Tamer  MedhatDownload Free PDFView PDFPointing Task Evaluation of Leap Motion Controller in 3D Virtual EnvironmentJoanna  Camargo, Fons VerbeekDownload Free PDFView PDFProposal and evaluation of a tablet-based tool for 3D virtual environmentsPriscilla BrazDownload Free PDFView PDF3D Imaging for Hand Gesture Recognition: Exploring The Software-Hardware Interaction of Current TechnologiesFrol PeriverzovDownload Free PDFView PDFA Multitouchless Interface: Expanding User InteractionRichard Bowden, Andrew GilbertDownload Free PDFView PDFAdvanced Visualization and Intuitive User Interface Systems for Biomedical Applicationsbasilis agDownload Free PDFView PDFTowards Wearable Attention-Aware Systems in Everyday Environments - Takumi Toyama2015 • Peter KurukaviDownload Free PDFView PDFReview of Virtual Environment Interface Technology1996 • Rob JohnstonDownload Free PDFView PDFNew wireless connection between user and VE using speech processing2014 • Jean-Rémy Chardonnet, James OliverDownload Free PDFView PDFMultimodality with Eye tracking and Haptics: A New Horizon for Serious Games?Shujie Deng, Julie Kirkby, J. ChangDownload Free PDFView PDFSensorsExploring Direct 3D Interaction for Full Horizontal Parallax Light Field Displays Using Leap Motion Controller2015 • Peter Szolgay, Vamsi AdhikarlaDownload Free PDFView PDFXD-AR: Challenges and Opportunities in Cross-Device Augmented Reality Application DevelopmentMaximilian SpeicherDownload Free PDFView PDFDESIGNING FOR ENERGY- EFFICIENT VISION-BASED INTERACTIVITY ON MOBILE DEVICESMiguel BordalloDownload Free PDFView PDF2013 XV Symposium on Virtual and Augmented RealityCorrecting User's Head and Body Orientation Using a Comfort Pose Function2013 • Anderson MacielDownload Free PDFView PDFA Sensing Architecture for Empathetic Data SystemsPaul VerschureDownload Free PDFView PDFSECCRIT: Secure Cloud Computing for High Assurance ServicesPaul SmithDownload Free PDFView PDFExtended multitouch: recovering touch posture and differentiating users using a depth camera2012 • Niklas ElmqvistDownload Free PDFView PDFThe Perceptive Workbench: Towards Spontaneous and Natural Interaction in Semi-Immersive Virtual Environments2000 • Thad StarnerDownload Free PDFView PDFHandbook of Virtual EnvironmentsPrinciples for the design of performance-oriented interaction techniques2002 • Doug BowmanDownload Free PDFView PDFLow-Cost Motion-Tracking for Computational Psychometrics based on Virtual RealityIrene Alice  Chicchi Giglioli, Pietro CipressoDownload Free PDFView PDFCo Kl Ma Ba13Bruno PDownload Free PDFView PDFRELATED TOPICSVirtual EnvironmentsUser Experience (UX)Wearable ComputingUser Centred DesignWearable TechnologiesUsability and user experienceGesture RecognitionCAVE (Cave Automatic Virtual Env...See Full PDFDownload PDFAboutPressBlogPeoplePapersTopicsJob Board We're Hiring! Help CenterFind new research papers in:PhysicsChemistryBiologyHealth SciencesEcologyEarth SciencesCognitive ScienceMathematicsComputer ScienceTermsPrivacyCopyrightAcademia ©2023