From duct tape to puppets: How a new data center became an opportunity to do things right – Diff
 
Skip to content
 
Diff
Categories
Equity & Inclusion
Education & Open Access
Technology
Partnerships & Events
Policy & Advocacy
Movement Strategy
About
Submit
Calendar
Wikimania 2023
 
Choose a language
English
Search 
From duct tape to puppets: How a new data center became an opportunity to do things right 
1 February 2013 by Sumana Harihareswara 
Translate This Post
Last week, the Wikimedia Foundation flipped a historic switch: we transitioned our main technical services to a shiny new data center in Ashburn, Virginia. For the first time since 2004, Wikimedia sites are no longer primarily hosted in Tampa, Florida.
Peter Youngmeister works in the Wikimedia Foundation’s Technical Operations team.
To help understand this grueling journey (and why it’s crucial), look through the eyes of Wikimedia Foundation engineer Peter Youngmeister. Peter joined the Wikimedia Foundation’s Technical Operations team (“Ops”) about two years ago, in March 2011. At the time, “the team” meant “about six engineers supporting the fifth-most visited site on the Web,” said Peter. The Foundation has now increased its Ops team to 14, and has several job openings.
“This also meant that out of the fast/cheap/well triangle, we’d gone with fast and cheap,” Peter recalled. We made quick-and-dirty solutions because problems had to be solved immediately. “With so few Ops engineers, you’re always playing catchup; long-term is hard.” He said that the digital infrastructure when he arrived was “kinda like many many layers of really artfully applied duct tape.”
And the biggest, most pressing flaw: Wikimedia only had one fully functional primary data center, in Tampa, Florida. If something catastrophic happened to Tampa, all the sites would go down until new servers could be brought online and data recovered from backup. So the Ops team chose a new data center location, in Ashburn, Virginia, and started preparing to integrate it into our infrastructure. But the preparation of EQIAD, which began in 2011, turned out to require much more work than the Operations and Platform engineering teams had foreseen.
We had never set up a data center of this complexity from scratch before. The systems in Tampa were “layers of duct tape that had been built up over years… Our first problem was that, for example, very little was in Puppet,” Peter said. To configure the Wikimedia servers, we use Puppet, a configuration management system, which lets us write code (Puppet “manifests”) that manages all of our servers like a single large application (and more easily track, troubleshoot, and revert changes).
Since the new data center would exactly mirror the old one, leveraging the power of Puppet to keep our configurations in sync would be crucial. But since our infrastructure included dozens of services that weren’t in Puppet yet, we had to examine each of their configurations to “puppetize” them. And in early 2011, Peter noted, “our whole search infrastructure existed outside of Puppet control. Our Puppet manifests for our databases were a file that just had a comment that said ‘domas is a slacker.'”
In short, Wikimedia needed not only to replicate the functionality that had been incrementally added over ten years, but to refactor it into an automatable form so that the third, fourth, etc. replications would be far easier. So, in addition to the Ops team’s day-to-day responsibilities for site maintenance and crisis management, Ops and Platform teams needed to find hundreds or thousands of staff-hours to refactor, automate and add monitoring to all the services it provided. We aren’t done yet with our “mass puppetization” investment, which we’ve been working on for at least two years.
The core application (MediaWiki) is only one of the myriad moving parts that needed attention; over the past two years, we’ve puppetized and strengthened databases, search, fundraising code, logging and analytics tools, caches, the Nagios monitoring software and dozens of other services. Take search as an example: several years ago, the Wikimedia Foundation used one search server to cover nearly all the wikis other than English Wikipedia — a dangerous single point of failure. Peter arrived at the Foundation and found that none of the search infrastructure was puppetized. After he worked significantly on search, as of November 2012, he noted we had “two fully independent search setups, one in each data center. Fail-over takes a couple of minutes at most.”
Puppet Tutorial: Video from the Wikimedia Foundation tech days, September 11, 2012, explaining Puppet configuration management in the context of Wikimedia’s site/services infrastructure. Speaker/slides: Ryan Lane.
Puppetizing the configuration files, and using Gerrit to manage code review and approval also gave us better transparency and helped staff and volunteers collaborate better on improvements, maintenance and troubleshooting. Anyone can see how our servers are configured, read the Puppet configuration “manifests,” propose new changes and view and comment on pending proposals.
In contrast, “when I got here, everything was done on a local Subversion repository or our puppetmaster, and then pushed out from there, which kinda works if you have 6 or fewer people,” Peter said. (The Puppetmaster is the master repository that instructs all the other boxes in the cluster to update their manifests, and thus updates their packages and configurations.) To keep track of configuration changes, people simply used an IRC bot to log summaries of their actions to the server admin log, which made it hard to revert changes or help train new teammates. “But also, when the Ops team is only 6 people, and everyone has been around for years, everyone just knows all the parts,” he explained.
As they created the 700+ hostclasses currently defined in Puppet, Operations engineers moved towards treating our infrastructure as a codebase, and thus from pure systems administration towards a DevOps approach. As of November 2012, “we’re very nearly at a point where we can manage our whole infrastructure without needing to log into hosts, which is the whole goal,” Peter said with a smile. Logging into hosts is a bad thing “because it means that you’re doing things by hand and/or that what you’re doing isn’t going through code review. Moving to Gerrit for our Puppet repos is awesome: It means I can really easily see what my coworkers are doing. I can ask for review when needed. It’s a huge sign of maturation of our department.”
Their years of work have led to a nearly painless data center migration, but it also began paying off immediately with reduced downtime. You’ll read more about that in the second part of this story next week.
Sumana Harihareswara, Engineering Community Manager
Archive notice: This is an archived post from blog.wikimedia.org, which operated under different editorial and content guidelines than Diff.
 
Can you help us translate this article?
In order for this article to reach as many people as possible we would like your help. Can you translate this article to get the message out?
Start translation
Related
No comments
Comments are closed automatically after 21 days.
Meta
Posted in Operations, TechnologyTagged Ashburn, data center, Tampa, Wikimedia Blog (EN Archive) 
Related
 
Welcome to Diff
Welcome to Diff, a community blog by – and for – the Wikimedia movement. Join Diff today to share stories from your community and comment on articles. We want to hear your voice!
Learn more
Subscribe to Diff via Email
Enter your email address to subscribe to Diff and receive notifications of new posts by email.
Email Address
Subscribe
 
Upcoming Events
Sep
15
15 September @ 10:00 - 17 September @ 17:00  UTC  
Wikimedia CEE Meeting 2023
Organizer: 
Wikimedia Georgia
Oct
6
15:00 - 16:00  UTC  
Event Series
Let’s Connect Connectathon
Organizer: 
Let's Connect working group
Oct
20
20 October @ 03:30 - 22 October @ 10:30  UTC  
WikiWomenCamp 2023
Organizer: 
Chinmayee Mishra & Manavpreet Kaur
View Calendar
Wikimedia News
Wikimedia Foundation News
Wikimania 2023 in Singapore centers on diversity, collaboration, and the future of free knowledge10 August 2023 by Wikimedia Foundation
Wikimedia Technology Blog
Flame graphs arrive in WikimediaDebug8 June 2023 by Timo Tijhof
Down the Rabbit Hole
Wikipedia’s value in the age of generative AI12 July 2023 by Selena Deckelmann
 
Diff This is Diff, a Wikimedia community blog.
All participants are responsible for building a place that is welcoming and friendly to everyone. Learn more about Diff.
A Wikimedia Foundation Project
Links 
Join
Subscribe
Guidelines
Editorial guidelines
Privacy Policy
Terms of Use
Log in
Content licensed under Creative Commons Attribution-ShareAlike 4.0 (CC-BY-SA) unless otherwise noted.
Powered by WordPress.com VIP, Automattic Privacy Notice. 